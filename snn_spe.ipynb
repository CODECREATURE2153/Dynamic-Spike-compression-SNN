{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import snntorch as snn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from snntorch import surrogate\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "num_steps = 100  # 1 second at 10 ms frames\n",
    "num_inputs = 1300  # 13 MFCCs Ã— 100 frames\n",
    "num_hidden = 100\n",
    "num_outputs = 10  # 10 command words\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 15\n",
    "patience = 3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check dataset\n",
    "dataset_dir = \"speech_commands_v0.02\"\n",
    "if not os.path.exists(dataset_dir):\n",
    "    print(\"Please download speech_commands_v0.02.tar.gz from http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz and extract to 'speech_commands_v0.02'\")\n",
    "    exit()\n",
    "\n",
    "# Custom Dataset for Google Speech Commands\n",
    "class SpeechCommandsDataset(Dataset):\n",
    "    def __init__(self, root=\"speech_commands_v0.02\", train=True, target_words=[\"yes\", \"no\", \"up\", \"down\", \"left\", \"right\", \"on\", \"off\", \"stop\", \"go\"]):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.target_words = target_words\n",
    "        self.transform = torchaudio.transforms.MFCC(\n",
    "            sample_rate=16000,\n",
    "            n_mfcc=13,\n",
    "            melkwargs={\n",
    "                'n_fft': 400,\n",
    "                'hop_length': 160,\n",
    "                'f_min': 20,\n",
    "                'f_max': 4000,\n",
    "                'n_mels': 40\n",
    "            }\n",
    "        )\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for word in os.listdir(root):\n",
    "            if word in target_words:\n",
    "                word_dir = os.path.join(root, word)\n",
    "                for file in os.listdir(word_dir):\n",
    "                    if file.endswith(\".wav\"):\n",
    "                        file_path = os.path.join(word_dir, file)\n",
    "                        label = target_words.index(word)\n",
    "                        self.data.append(file_path)\n",
    "                        self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.data[idx], self.labels[idx]\n",
    "        try:\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "            if waveform.numel() == 0:\n",
    "                raise ValueError(\"Empty waveform\")\n",
    "            if sample_rate != 16000:\n",
    "                resampler = torchaudio.transforms.Resample(sample_rate, 16000)\n",
    "                waveform = resampler(waveform)\n",
    "            mfcc = self.transform(waveform)\n",
    "            if mfcc.dim() == 3 and mfcc.shape[0] == 1:\n",
    "                mfcc = mfcc.squeeze(0)  # Shape: [13, num_frames]\n",
    "            num_frames = mfcc.shape[1]\n",
    "            if num_frames < 100:\n",
    "                pad_width = 100 - num_frames\n",
    "                mfcc = torch.nn.functional.pad(mfcc, (0, pad_width))\n",
    "            elif num_frames > 100:\n",
    "                mfcc = mfcc[:, :100]\n",
    "            flattened = mfcc.flatten()  # Shape: [1300]\n",
    "            return flattened, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            return torch.zeros(1300), -1  # Dummy tensor and label\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    valid_indices = [i for i, target in enumerate(targets) if target != -1]\n",
    "    if not valid_indices:\n",
    "        return torch.zeros(1, 1300), torch.zeros(1, dtype=torch.long)\n",
    "    data = [data[i] for i in valid_indices]\n",
    "    targets = [targets[i] for i in valid_indices]\n",
    "    data = torch.stack(data)\n",
    "    targets = torch.tensor(targets)\n",
    "    return data, targets\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = SpeechCommandsDataset(train=True)\n",
    "test_dataset = SpeechCommandsDataset(train=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Surrogate gradient\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)\n",
    "\n",
    "# Baseline SNN\n",
    "class BaselineSNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif1 = snn.Leaky(beta=0.9, spike_grad=spike_grad)\n",
    "        self.lif2 = snn.Leaky(beta=0.9, spike_grad=spike_grad)\n",
    "        for layer in [self.fc1, self.fc2]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        mem1, mem2 = self.lif1.init_leaky(), self.lif2.init_leaky()\n",
    "        spk1_rec, spk2_rec = [], []\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x[step])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "        return torch.stack(spk1_rec, dim=0), torch.stack(spk2_rec, dim=0)\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "net = BaselineSNN().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.2)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "best_val_accuracy = 0.0\n",
    "epochs_no_improve = 0\n",
    "accuracies, val_accuracies, losses, total_spikes_list = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    total_spikes = 0\n",
    "    for data, targets in train_loader:\n",
    "        valid_indices = targets != -1\n",
    "        if not valid_indices.any():\n",
    "            continue\n",
    "        data = data[valid_indices]\n",
    "        targets = targets[valid_indices]\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        batch_size_actual = data.size(0)\n",
    "        if data.max() > 0:\n",
    "            data = data / data.max()\n",
    "        spike_data = (torch.rand(num_steps, batch_size_actual, num_inputs, device=device) < data * 0.2).float()\n",
    "        spk1_rec, outputs = net(spike_data)\n",
    "        spk_count = outputs.sum(dim=0)\n",
    "        loss = criterion(spk_count, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=3.0)\n",
    "        optimizer.step()\n",
    "        correct = (spk_count.argmax(dim=1) == targets).sum().item()\n",
    "        accuracy = correct / batch_size_actual\n",
    "        total_spikes += (spk1_rec.sum() + outputs.sum()).item()\n",
    "    scheduler.step()\n",
    "    losses.append(loss.item())\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    val_total_spikes = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            valid_indices = targets != -1\n",
    "            if not valid_indices.any():\n",
    "                continue\n",
    "            data = data[valid_indices]\n",
    "            targets = targets[valid_indices]\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            batch_size_actual = data.size(0)\n",
    "            if data.max() > 0:\n",
    "                data = data / data.max()\n",
    "            spike_data = (torch.rand(num_steps, batch_size_actual, num_inputs, device=device) < data * 0.2).float()\n",
    "            spk1_rec, outputs = net(spike_data)\n",
    "            spk_count = outputs.sum(dim=0)\n",
    "            total_correct += (spk_count.argmax(dim=1) == targets).sum().item()\n",
    "            total_samples += batch_size_actual\n",
    "            val_total_spikes += (spk1_rec.sum() + outputs.sum()).item()\n",
    "        val_accuracy = total_correct / total_samples if total_samples > 0 else 0.0\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        total_spikes_list.append(val_total_spikes)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}, Total Spikes: {val_total_spikes:.0f}\")\n",
    "\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(losses, label=\"Loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(total_spikes_list, label=\"Total Spikes\")\n",
    "plt.title(\"Total Spikes\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"baseline_snn_results.png\")\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "final_val_accuracy = max(val_accuracies) if val_accuracies else 0.0\n",
    "final_total_spikes = total_spikes_list[val_accuracies.index(final_val_accuracy)] if val_accuracies else 0\n",
    "energy = final_total_spikes * 20 / 1e6  # ÂµJ\n",
    "print(\"\\nBaseline SNN Final Results:\")\n",
    "print(f\"Peak Validation Accuracy: {final_val_accuracy:.2%}\")\n",
    "print(f\"Total Spikes at Peak Accuracy: {final_total_spikes:.0f}\")\n",
    "print(f\"Energy Consumption: {energy:.2f} ÂµJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.3626, Accuracy: 0.1042, Validation Accuracy: 0.1059, Avg Spike Rate: 0.0047\n",
      "Epoch 2, Loss: 2.3046, Accuracy: 0.1070, Validation Accuracy: 0.1118, Avg Spike Rate: 0.0001\n",
      "Epoch 3, Loss: 2.3026, Accuracy: 0.1069, Validation Accuracy: 0.1110, Avg Spike Rate: 0.0000\n",
      "Epoch 4, Loss: 2.3026, Accuracy: 0.1054, Validation Accuracy: 0.1080, Avg Spike Rate: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 131\u001b[0m\n\u001b[0;32m    128\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    129\u001b[0m running_loss, correct, total_spikes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 131\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 46\u001b[0m, in \u001b[0;36mSubsetSC.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     45\u001b[0m     waveform, sample_rate, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m---> 46\u001b[0m     mfcc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mfcc, class_to_idx[label]\n",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(waveform, sample_rate)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(waveform, sample_rate):\n\u001b[1;32m---> 23\u001b[0m     transform \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMFCC\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_mfcc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmelkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_fft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhop_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_mels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     mfcc \u001b[38;5;241m=\u001b[39m transform(waveform)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m     time_steps \u001b[38;5;241m=\u001b[39m mfcc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:683\u001b[0m, in \u001b[0;36mMFCC.__init__\u001b[1;34m(self, sample_rate, n_mfcc, dct_type, norm, log_mels, melkwargs)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamplitude_to_DB \u001b[38;5;241m=\u001b[39m AmplitudeToDB(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpower\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_db)\n\u001b[0;32m    682\u001b[0m melkwargs \u001b[38;5;241m=\u001b[39m melkwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m--> 683\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMelSpectrogram \u001b[38;5;241m=\u001b[39m \u001b[43mMelSpectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmelkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_mfcc \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMelSpectrogram\u001b[38;5;241m.\u001b[39mn_mels:\n\u001b[0;32m    686\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot select more MFCC coefficients than # mel bins\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:607\u001b[0m, in \u001b[0;36mMelSpectrogram.__init__\u001b[1;34m(self, sample_rate, n_fft, win_length, hop_length, f_min, f_max, pad, n_mels, window_fn, power, normalized, wkwargs, center, pad_mode, onesided, norm, mel_scale)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_min \u001b[38;5;241m=\u001b[39m f_min\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrogram \u001b[38;5;241m=\u001b[39m Spectrogram(\n\u001b[0;32m    595\u001b[0m     n_fft\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fft,\n\u001b[0;32m    596\u001b[0m     win_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwin_length,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    605\u001b[0m     onesided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    606\u001b[0m )\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_scale \u001b[38;5;241m=\u001b[39m \u001b[43mMelScale\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_scale\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:399\u001b[0m, in \u001b[0;36mMelScale.__init__\u001b[1;34m(self, n_mels, sample_rate, f_min, f_max, n_stft, norm, mel_scale)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f_min \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_max:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequire f_min: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m <= f_max: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(f_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_max))\n\u001b[1;32m--> 399\u001b[0m fb \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelscale_fbanks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_stft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_mels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmel_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_buffer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfb\u001b[39m\u001b[38;5;124m\"\u001b[39m, fb)\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\functional\\functional.py:576\u001b[0m, in \u001b[0;36mmelscale_fbanks\u001b[1;34m(n_freqs, f_min, f_max, n_mels, sample_rate, norm, mel_scale)\u001b[0m\n\u001b[0;32m    573\u001b[0m f_pts \u001b[38;5;241m=\u001b[39m _mel_to_hz(m_pts, mel_scale\u001b[38;5;241m=\u001b[39mmel_scale)\n\u001b[0;32m    575\u001b[0m \u001b[38;5;66;03m# create filterbank\u001b[39;00m\n\u001b[1;32m--> 576\u001b[0m fb \u001b[38;5;241m=\u001b[39m \u001b[43m_create_triangular_filterbank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_freqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_pts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslaney\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;66;03m# Slaney-style mel is scaled to be approx constant energy per channel\u001b[39;00m\n\u001b[0;32m    580\u001b[0m     enorm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m \u001b[38;5;241m/\u001b[39m (f_pts[\u001b[38;5;241m2\u001b[39m : n_mels \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m f_pts[:n_mels])\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\functional\\functional.py:516\u001b[0m, in \u001b[0;36m_create_triangular_filterbank\u001b[1;34m(all_freqs, f_pts)\u001b[0m\n\u001b[0;32m    514\u001b[0m down_slopes \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m slopes[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;241m/\u001b[39m f_diff[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# (n_freqs, n_filter)\u001b[39;00m\n\u001b[0;32m    515\u001b[0m up_slopes \u001b[38;5;241m=\u001b[39m slopes[:, \u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m/\u001b[39m f_diff[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# (n_freqs, n_filter)\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m fb \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzero\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdown_slopes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup_slopes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fb\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Constants\n",
    "data_path = './speechcommands_data'\n",
    "target_classes = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "class_to_idx = {label: idx for idx, label in enumerate(target_classes)}\n",
    "FIXED_MFCC_LENGTH = 100\n",
    "\n",
    "# Preprocessing\n",
    "def preprocess(waveform, sample_rate):\n",
    "    transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=20,\n",
    "        melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 64}\n",
    "    )\n",
    "    mfcc = transform(waveform).squeeze(0)\n",
    "    time_steps = mfcc.shape[1]\n",
    "\n",
    "    if time_steps < FIXED_MFCC_LENGTH:\n",
    "        pad_amt = FIXED_MFCC_LENGTH - time_steps\n",
    "        mfcc = F.pad(mfcc, (0, pad_amt))\n",
    "    else:\n",
    "        mfcc = mfcc[:, :FIXED_MFCC_LENGTH]\n",
    "\n",
    "    return mfcc\n",
    "\n",
    "# Dataset\n",
    "class SubsetSC(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = [(waveform, sample_rate, label) for waveform, sample_rate, label, *_ in samples if label in target_classes]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sample_rate, label = self.samples[index]\n",
    "        mfcc = preprocess(waveform, sample_rate)\n",
    "        return mfcc, class_to_idx[label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# Load Data\n",
    "full_train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_path, download=True, subset='training')\n",
    "full_val_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_path, download=True, subset='validation')\n",
    "\n",
    "train_dataset = SubsetSC(full_train_dataset)\n",
    "val_dataset = SubsetSC(full_val_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# ðŸ”¥ Improved LIF Neuron with sharper surrogate and optional dropout\n",
    "class LIFNeuron(nn.Module):\n",
    "    def __init__(self, tau=2.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "        self.dropout = nn.Dropout(dropout) if dropout > 0 else None\n",
    "\n",
    "    def forward(self, x, mem):\n",
    "        mem = mem * self.tau + x\n",
    "        spk = self.surrogate_spike(mem - 1.0)\n",
    "        mem = mem * (1.0 - spk)\n",
    "        if self.dropout is not None:\n",
    "            spk = self.dropout(spk)\n",
    "        return spk, mem\n",
    "\n",
    "    def surrogate_spike(self, x):\n",
    "        return torch.sigmoid(10 * x)  # ðŸ”¥ steeper surrogate\n",
    "\n",
    "    def init_mem(self, batch_size, features, device):\n",
    "        return torch.zeros(batch_size, features, device=device)\n",
    "\n",
    "# ðŸ”¥ Improved SNN Model with LayerNorm\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)  # ðŸ”¥ normalization\n",
    "        self.lif1 = LIFNeuron(dropout=0.2)       # ðŸ”¥ slight dropout\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.lif2 = LIFNeuron()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, steps, features = x.shape\n",
    "        spk2_rec = []\n",
    "        mem1 = self.lif1.init_mem(batch_size, self.fc1.out_features, x.device)\n",
    "        mem2 = self.lif2.init_mem(batch_size, self.fc2.out_features, x.device)\n",
    "\n",
    "        for step in range(steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            cur1 = self.norm1(cur1)        # ðŸ”¥ normalize activations\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "\n",
    "        return torch.stack(spk2_rec)\n",
    "\n",
    "# ðŸ”¥ Rate encoding instead of Poisson\n",
    "def rate_encode(x, num_steps):\n",
    "    x = x.unsqueeze(0).expand(num_steps, *x.shape)\n",
    "    return (torch.rand_like(x) < x).float()\n",
    "\n",
    "# Config\n",
    "input_size = 20 * FIXED_MFCC_LENGTH\n",
    "hidden_size = 512     # ðŸ”¥ bigger hidden size\n",
    "output_size = 10\n",
    "num_epochs = 15       # ðŸ”¥ longer training\n",
    "num_steps = 20\n",
    "\n",
    "net = SNN(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss, correct, total_spikes = 0, 0, 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs / inputs.max()\n",
    "        inputs = inputs.view(inputs.size(0), inputs.size(2), inputs.size(1))\n",
    "\n",
    "        spk_in = rate_encode(inputs, num_steps)\n",
    "        spk_in = spk_in.permute(1, 0, 2, 3)\n",
    "        spk_in = spk_in.reshape(spk_in.size(0), spk_in.size(1), -1)\n",
    "\n",
    "        spk2_rec = net(spk_in)\n",
    "        spk_count = spk2_rec.sum(0) / num_steps\n",
    "\n",
    "        loss = criterion(spk_count, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct += (spk_count.argmax(1) == targets).sum().item()\n",
    "        total_spikes += spk2_rec.sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_spike_rate = total_spikes / (len(train_loader.dataset) * num_steps * hidden_size)\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs / inputs.max()\n",
    "            inputs = inputs.view(inputs.size(0), inputs.size(2), inputs.size(1))\n",
    "\n",
    "            spk_in = rate_encode(inputs, num_steps)\n",
    "            spk_in = spk_in.permute(1, 0, 2, 3)\n",
    "            spk_in = spk_in.reshape(spk_in.size(0), spk_in.size(1), -1)\n",
    "\n",
    "            spk2_rec = net(spk_in)\n",
    "            spk_count = spk2_rec.sum(0) / num_steps\n",
    "\n",
    "            correct += (spk_count.argmax(1) == targets).sum().item()\n",
    "\n",
    "    val_acc = correct / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}, Avg Spike Rate: {avg_spike_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 9.0104, Accuracy: 0.0982, Validation Accuracy: 0.1007, Avg Spike Rate: 0.0334\n",
      "Epoch 2, Loss: 7.1013, Accuracy: 0.0977, Validation Accuracy: 0.1018, Avg Spike Rate: 0.0351\n",
      "Epoch 3, Loss: 7.1014, Accuracy: 0.1018, Validation Accuracy: 0.1018, Avg Spike Rate: 0.0351\n",
      "Epoch 4, Loss: 7.0994, Accuracy: 0.0969, Validation Accuracy: 0.0945, Avg Spike Rate: 0.0351\n",
      "Epoch 5, Loss: 7.1025, Accuracy: 0.0957, Validation Accuracy: 0.0945, Avg Spike Rate: 0.0351\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 140\u001b[0m\n\u001b[0;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(spk_count, targets)\n\u001b[0;32m    139\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 140\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(net\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    142\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Constants\n",
    "data_path = './speechcommands_data'\n",
    "target_classes = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "class_to_idx = {label: idx for idx, label in enumerate(target_classes)}\n",
    "FIXED_MFCC_LENGTH = 100\n",
    "\n",
    "# Preprocessing with proper normalization\n",
    "def preprocess(waveform, sample_rate):\n",
    "    transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=20,\n",
    "        melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 64}\n",
    "    )\n",
    "    mfcc = transform(waveform).squeeze(0)\n",
    "    time_steps = mfcc.shape[1]\n",
    "\n",
    "    if time_steps < FIXED_MFCC_LENGTH:\n",
    "        pad_amt = FIXED_MFCC_LENGTH - time_steps\n",
    "        mfcc = F.pad(mfcc, (0, pad_amt))\n",
    "    else:\n",
    "        mfcc = mfcc[:, :FIXED_MFCC_LENGTH]\n",
    "\n",
    "    # Shift and scale MFCCs to [0, 1]\n",
    "    mfcc = (mfcc - mfcc.min()) / (mfcc.max() - mfcc.min() + 1e-8)\n",
    "    return mfcc\n",
    "\n",
    "# Dataset\n",
    "class SubsetSC(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = [(waveform, sample_rate, label) for waveform, sample_rate, label, *_ in samples if label in target_classes]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sample_rate, label = self.samples[index]\n",
    "        mfcc = preprocess(waveform, sample_rate)\n",
    "        return mfcc, class_to_idx[label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# Load Data\n",
    "full_train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_path, download=True, subset='training')\n",
    "full_val_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_path, download=True, subset='validation')\n",
    "\n",
    "train_dataset = SubsetSC(full_train_dataset)\n",
    "val_dataset = SubsetSC(full_val_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# LIF Neuron with sharper surrogate\n",
    "class LIFNeuron(nn.Module):\n",
    "    def __init__(self, tau=2.0):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, x, mem):\n",
    "        mem = mem * self.tau + x\n",
    "        spk = self.surrogate_spike(mem - 1.0)\n",
    "        mem = mem * (1.0 - spk)\n",
    "        return spk, mem\n",
    "\n",
    "    def surrogate_spike(self, x):\n",
    "        return torch.sigmoid(10 * x)  # Steeper surrogate\n",
    "\n",
    "    def init_mem(self, batch_size, features, device):\n",
    "        return torch.zeros(batch_size, features, device=device)\n",
    "\n",
    "# Simplified SNN Model without LayerNorm or Dropout\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lif1 = LIFNeuron()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.lif2 = LIFNeuron()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, steps, features = x.shape\n",
    "        spk2_rec = []\n",
    "        mem1 = self.lif1.init_mem(batch_size, self.fc1.out_features, x.device)\n",
    "        mem2 = self.lif2.init_mem(batch_size, self.fc2.out_features, x.device)\n",
    "\n",
    "        for step in range(steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "\n",
    "        return torch.stack(spk2_rec)\n",
    "\n",
    "# Rate encoding with increased time steps\n",
    "num_steps = 50  # Increased from 20 to 50\n",
    "def rate_encode(x, num_steps):\n",
    "    x = x.unsqueeze(0).expand(num_steps, *x.shape)\n",
    "    return (torch.rand_like(x) < x).float()\n",
    "\n",
    "# Config\n",
    "input_size = 20 * FIXED_MFCC_LENGTH\n",
    "hidden_size = 256  # Adjusted hidden size\n",
    "output_size = 10\n",
    "num_epochs = 15\n",
    "net = SNN(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)  # Slower decay\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss, correct, total_spikes = 0, 0, 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.view(inputs.size(0), inputs.size(2), inputs.size(1))  # [batch_size, num_frames, num_mfcc]\n",
    "\n",
    "        spk_in = rate_encode(inputs, num_steps)  # [num_steps, batch_size, num_frames, num_mfcc]\n",
    "        spk_in = spk_in.permute(1, 0, 2, 3)  # [batch_size, num_steps, num_frames, num_mfcc]\n",
    "        spk_in = spk_in.reshape(spk_in.size(0), spk_in.size(1), -1)  # [batch_size, num_steps, input_size]\n",
    "\n",
    "        spk2_rec = net(spk_in)\n",
    "        spk_count = spk2_rec.sum(0)  # Sum over time steps: [batch_size, num_classes]\n",
    "\n",
    "        loss = criterion(spk_count, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct += (spk_count.argmax(1) == targets).sum().item()\n",
    "        total_spikes += spk2_rec.sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_spike_rate = total_spikes / (len(train_loader.dataset) * num_steps * hidden_size)\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.view(inputs.size(0), inputs.size(2), inputs.size(1))\n",
    "\n",
    "            spk_in = rate_encode(inputs, num_steps)\n",
    "            spk_in = spk_in.permute(1, 0, 2, 3)\n",
    "            spk_in = spk_in.reshape(spk_in.size(0), spk_in.size(1), -1)\n",
    "\n",
    "            spk2_rec = net(spk_in)\n",
    "            spk_count = spk2_rec.sum(0)  # Sum over time steps\n",
    "\n",
    "            correct += (spk_count.argmax(1) == targets).sum().item()\n",
    "\n",
    "    val_acc = correct / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}, Avg Spike Rate: {avg_spike_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 62.5 KiB for an array with shape (16000, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m full_train_dataset \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mSPEECHCOMMANDS(root\u001b[38;5;241m=\u001b[39mdata_path, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m full_val_dataset \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mSPEECHCOMMANDS(root\u001b[38;5;241m=\u001b[39mdata_path, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSubsetSC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_train_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m SubsetSC(full_val_dataset)\n\u001b[0;32m     67\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mSubsetSC.__init__\u001b[1;34m(self, samples)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_classes\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, samples):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_classes\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\datasets\\speechcommands.py:179\u001b[0m, in \u001b[0;36mSPEECHCOMMANDS.__getitem__\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load the n-th sample from the dataset.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[0;32m    161\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m        Utterance number\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    178\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metadata(n)\n\u001b[1;32m--> 179\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43m_load_waveform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_archive\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (waveform,) \u001b[38;5;241m+\u001b[39m metadata[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\datasets\\utils.py:51\u001b[0m, in \u001b[0;36m_load_waveform\u001b[1;34m(root, filename, exp_sample_rate)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_waveform\u001b[39m(\n\u001b[0;32m     46\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     47\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     48\u001b[0m     exp_sample_rate: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m     49\u001b[0m ):\n\u001b[0;32m     50\u001b[0m     path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, filename)\n\u001b[1;32m---> 51\u001b[0m     waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exp_sample_rate \u001b[38;5;241m!=\u001b[39m sample_rate:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample rate should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_sample_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\_backend\\utils.py:205\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03mBy default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m        `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    204\u001b[0m backend \u001b[38;5;241m=\u001b[39m dispatcher(uri, \u001b[38;5;28mformat\u001b[39m, backend)\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\_backend\\soundfile.py:27\u001b[0m, in \u001b[0;36mSoundfileBackend.load\u001b[1;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     19\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     buffer_size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4096\u001b[39m,\n\u001b[0;32m     26\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msoundfile_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\_backend\\soundfile_backend.py:230\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[0;32m    227\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m _SUBTYPE2DTYPE[file_\u001b[38;5;241m.\u001b[39msubtype]\n\u001b[0;32m    229\u001b[0m     frames \u001b[38;5;241m=\u001b[39m file_\u001b[38;5;241m.\u001b[39m_prepare_read(frame_offset, \u001b[38;5;28;01mNone\u001b[39;00m, num_frames)\n\u001b[1;32m--> 230\u001b[0m     waveform \u001b[38;5;241m=\u001b[39m \u001b[43mfile_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     sample_rate \u001b[38;5;241m=\u001b[39m file_\u001b[38;5;241m.\u001b[39msamplerate\n\u001b[0;32m    233\u001b[0m waveform \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(waveform)\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\soundfile.py:938\u001b[0m, in \u001b[0;36mSoundFile.read\u001b[1;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m     frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_frames(frames, fill_value)\n\u001b[1;32m--> 938\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_empty_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m frames \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(out):\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\soundfile.py:1372\u001b[0m, in \u001b[0;36mSoundFile._create_empty_array\u001b[1;34m(self, frames, always_2d, dtype)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1371\u001b[0m     shape \u001b[38;5;241m=\u001b[39m frames,\n\u001b[1;32m-> 1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mempty(shape, dtype, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 62.5 KiB for an array with shape (16000, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Constants\n",
    "data_path = './speechcommands_data'\n",
    "target_classes = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
    "class_to_idx = {label: idx for idx, label in enumerate(target_classes)}\n",
    "FIXED_MFCC_LENGTH = 100\n",
    "num_steps = 100  # Match the number of MFCC frames\n",
    "num_mfcc = 20  # Number of MFCC coefficients per frame\n",
    "num_epochs = 10  # Define number of epochs\n",
    "\n",
    "# Preprocessing with proper normalization\n",
    "def preprocess(waveform, sample_rate):\n",
    "    transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=num_mfcc,\n",
    "        melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 64}\n",
    "    )\n",
    "    mfcc = transform(waveform).squeeze(0)  # Shape: [num_mfcc, num_frames]\n",
    "    time_steps = mfcc.shape[1]\n",
    "\n",
    "    if time_steps < FIXED_MFCC_LENGTH:\n",
    "        pad_amt = FIXED_MFCC_LENGTH - time_steps\n",
    "        mfcc = F.pad(mfcc, (0, pad_amt))\n",
    "    else:\n",
    "        mfcc = mfcc[:, :FIXED_MFCC_LENGTH]\n",
    "\n",
    "    # Normalize MFCCs to [0, 1]\n",
    "    mfcc_min = mfcc.min()\n",
    "    mfcc_max = mfcc.max()\n",
    "    mfcc = (mfcc - mfcc_min) / (mfcc_max - mfcc_min + 1e-8)\n",
    "    return mfcc  # Shape: [num_mfcc, FIXED_MFCC_LENGTH]\n",
    "\n",
    "# Dataset\n",
    "class SubsetSC(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.samples = [(waveform, sample_rate, label) for waveform, sample_rate, label, *_ in samples if label in target_classes]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        waveform, sample_rate, label = self.samples[index]\n",
    "        mfcc = preprocess(waveform, sample_rate)  # Shape: [num_mfcc, FIXED_MFCC_LENGTH]\n",
    "        mfcc = mfcc.transpose(0, 1)  # Shape: [FIXED_MFCC_LENGTH, num_mfcc]\n",
    "        return mfcc, class_to_idx[label]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# Load Data\n",
    "full_train_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_path, download=True, subset='training')\n",
    "full_val_dataset = torchaudio.datasets.SPEECHCOMMANDS(root=data_path, download=True, subset='validation')\n",
    "\n",
    "train_dataset = SubsetSC(full_train_dataset)\n",
    "val_dataset = SubsetSC(full_val_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=False)\n",
    "\n",
    "# LIF Neuron with corrected tau and simple surrogate\n",
    "class LIFNeuron(nn.Module):\n",
    "    def __init__(self, tau=0.9):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, x, mem):\n",
    "        mem = mem * self.tau + x\n",
    "        spk = (mem >= 1.0).float()  # Simple step function for spikes\n",
    "        mem = mem * (1.0 - spk)  # Reset membrane potential\n",
    "        return spk, mem\n",
    "\n",
    "    def init_mem(self, batch_size, features, device):\n",
    "        return torch.zeros(batch_size, features, device=device)\n",
    "\n",
    "# SNN Model without LayerNorm or Dropout\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.lif1 = LIFNeuron(tau=0.9)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.lif2 = LIFNeuron(tau=0.9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, steps, features = x.shape\n",
    "        mem1 = self.lif1.init_mem(batch_size, self.fc1.out_features, x.device)\n",
    "        mem2 = self.lif2.init_mem(batch_size, self.fc2.out_features, x.device)\n",
    "        spk2_rec = []\n",
    "\n",
    "        for step in range(steps):\n",
    "            cur1 = self.fc1(x[:, step, :])\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk2_rec.append(spk2)\n",
    "\n",
    "        return torch.stack(spk2_rec, dim=0)  # [steps, batch_size, output_size]\n",
    "\n",
    "# Rate encoding for sequential input\n",
    "def rate_encode(mfcc, num_steps):\n",
    "    # mfcc: [batch_size, num_frames, num_mfcc]\n",
    "    batch_size, num_frames, num_mfcc = mfcc.shape\n",
    "    assert num_frames == num_steps, \"Number of frames must match num_steps\"\n",
    "    # Generate spikes for each frame\n",
    "    spikes = (torch.rand(batch_size, num_steps, num_mfcc, device=device) < mfcc).float()\n",
    "    return spikes  # [batch_size, num_steps, num_mfcc]\n",
    "\n",
    "# Config\n",
    "input_size = num_mfcc  # 20 MFCC coefficients per frame\n",
    "hidden_size = 256\n",
    "output_size = 10\n",
    "\n",
    "net = SNN(input_size, hidden_size, output_size).to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    running_loss, correct, total_spikes = 0, 0, 0\n",
    "\n",
    "    for mfcc, targets in train_loader:\n",
    "        mfcc, targets = mfcc.to(device), targets.to(device)  # mfcc: [batch_size, 100, 20]\n",
    "        batch_size_actual = mfcc.size(0)\n",
    "\n",
    "        # Rate encoding: generate spikes for each frame\n",
    "        spk_in = rate_encode(mfcc, num_steps)  # [batch_size, 100, 20]\n",
    "\n",
    "        # Forward pass\n",
    "        spk2_rec = net(spk_in)  # [100, batch_size, 10]\n",
    "        spk_count = spk2_rec.sum(0)  # [batch_size, 10]\n",
    "\n",
    "        loss = criterion(spk_count, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        correct += (spk_count.argmax(1) == targets).sum().item()\n",
    "        total_spikes += spk2_rec.sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_spike_rate = total_spikes / (len(train_loader.dataset) * num_steps * hidden_size)\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for mfcc, targets in val_loader:\n",
    "            mfcc, targets = mfcc.to(device), targets.to(device)\n",
    "            batch_size_actual = mfcc.size(0)\n",
    "\n",
    "            spk_in = rate_encode(mfcc, num_steps)\n",
    "            spk2_rec = net(spk_in)\n",
    "            spk_count = spk2_rec.sum(0)\n",
    "            correct += (spk_count.argmax(1) == targets).sum().item()\n",
    "\n",
    "    val_acc = correct / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}, Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}, Avg Spike Rate: {avg_spike_rate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Snn's\\sn\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 143\u001b[0m\n\u001b[0;32m    140\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m21\u001b[39m):\n\u001b[1;32m--> 143\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 112\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    110\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    111\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 112\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    113\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n\u001b[0;32m    114\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 98\u001b[0m, in \u001b[0;36mRadLIFNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m         out \u001b[38;5;241m=\u001b[39m layer(out)\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, nn\u001b[38;5;241m.\u001b[39mLinear):\n\u001b[1;32m---> 98\u001b[0m             out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mneurons\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m(out)\n\u001b[0;32m     99\u001b[0m     spike_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(out)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spike_sum \u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\nn\\modules\\container.py:295\u001b[0m, in \u001b[0;36mModuleList.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues())[idx])\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_abs_string_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[1;32md:\\Snn's\\sn\\Lib\\site-packages\\torch\\nn\\modules\\container.py:285\u001b[0m, in \u001b[0;36mModuleList._get_abs_string_index\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    283\u001b[0m idx \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mindex(idx)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of range\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    287\u001b[0m     idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of range"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets import SPEECHCOMMANDS\n",
    "import torchaudio\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------\n",
    "# 1. Dataset Setup\n",
    "# ---------------------\n",
    "class SubsetSC(SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(\"./\", download=True)\n",
    "        def load_list(filename):\n",
    "            with open(os.path.join(self._path, filename)) as f:\n",
    "                return [os.path.join(self._path, line.strip()) for line in f]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "\n",
    "labels = sorted(list(set(dat[2] for dat in SubsetSC(\"training\"))))\n",
    "label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "def label_to_tensor(label):\n",
    "    return torch.tensor(label_to_index[label])\n",
    "\n",
    "def pad_sequence(batch):\n",
    "    tensors, targets = zip(*batch)\n",
    "    tensors = [t.squeeze(0).t() for t in tensors]\n",
    "    tensors = nn.utils.rnn.pad_sequence(tensors, batch_first=True).permute(0, 2, 1)\n",
    "    targets = torch.stack(targets)\n",
    "    return tensors, targets\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [(torchaudio.transforms.MFCC()(waveform), label_to_tensor(label)) for waveform, _, label, *_ in batch]\n",
    "    return pad_sequence(batch)\n",
    "\n",
    "train_set = SubsetSC(\"training\")\n",
    "val_set = SubsetSC(\"validation\")\n",
    "test_set = SubsetSC(\"testing\")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_set, batch_size=64, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=64, collate_fn=collate_fn)\n",
    "\n",
    "# ---------------------\n",
    "# 2. RadLIF SNN Module\n",
    "# ---------------------\n",
    "class RadLIFNeuron(nn.Module):\n",
    "    def __init__(self, size, beta=0.9):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.beta = beta\n",
    "        self.register_buffer(\"membrane\", torch.zeros(size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.membrane = self.beta * self.membrane + input\n",
    "        out = (self.membrane > 1.0).float()\n",
    "        self.membrane *= (1 - out)\n",
    "        return out\n",
    "\n",
    "class RadLIFNet(nn.Module):\n",
    "    def __init__(self, input_size, layer_sizes, output_size, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_size = input_size\n",
    "        self.neurons = nn.ModuleList()\n",
    "\n",
    "        for h_size in layer_sizes:\n",
    "            layers.append(nn.Linear(in_size, h_size))\n",
    "            layers.append(nn.LayerNorm(h_size))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            self.neurons.append(RadLIFNeuron(h_size))\n",
    "            in_size = h_size\n",
    "\n",
    "        self.hidden = nn.Sequential(*layers)\n",
    "        self.output = nn.Linear(in_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, steps = x.shape\n",
    "        x = x.permute(2, 0, 1)  # T x B x C\n",
    "        spike_sum = torch.zeros(batch_size, len(labels), device=x.device)\n",
    "\n",
    "        for t in range(x.size(0)):\n",
    "            out = x[t]\n",
    "            for i, layer in enumerate(self.hidden):\n",
    "                out = layer(out)\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    out = self.neurons[i](out)\n",
    "            spike_sum += self.output(out)\n",
    "\n",
    "        return spike_sum / x.size(0)\n",
    "\n",
    "# ---------------------\n",
    "# 3. Training Functions\n",
    "# ---------------------\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        correct += (out.argmax(1) == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct += (out.argmax(1) == y).sum().item()\n",
    "            total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "# ---------------------\n",
    "# 4. Run Training\n",
    "# ---------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RadLIFNet(input_size=40, layer_sizes=[256, 256, 128], output_size=len(labels)).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, 21):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchaudio\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msparch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mradlif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RadLIFNet\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msparch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Trainer\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msparch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calculate_accuracy\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sparch'"
     ]
    }
   ],
   "source": [
    "# Baseline SNN for Google Speech Commands using SpArch\n",
    "\n",
    "# 1. Install dependencies\n",
    "# Make sure these are installed in your environment:\n",
    "# pip install torch torchaudio numpy matplotlib\n",
    "# Clone SpArch if not done:\n",
    "# git clone https://github.com/Intelligent-Computing-Lab-YZU/SpArch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "from sparch.models.radlif import RadLIFNet\n",
    "from sparch.utils.trainer import Trainer\n",
    "from sparch.utils.metrics import calculate_accuracy\n",
    "from sparch.utils.dataset import get_speechcommands_dataloaders\n",
    "\n",
    "# 2. Configuration\n",
    "config = {\n",
    "    'model_type': 'RadLIF',\n",
    "    'dataset_name': 'sc',\n",
    "    'data_folder': './speech_commands',\n",
    "    'batch_size': 64,\n",
    "    'nb_epochs': 30,\n",
    "    'lr': 1e-3,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'weight_decay': 1e-4,\n",
    "    'dropout': 0.2,\n",
    "    'layer_sizes': [256, 256, 128],\n",
    "    'bidirectional': True,\n",
    "    'normalization': 'layernorm',\n",
    "    'optimizer': 'adamw',\n",
    "    'scheduler': 'cosine',\n",
    "    't_max': 30,\n",
    "    'spike_tracking': True,\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "}\n",
    "\n",
    "# 3. Load dataset\n",
    "train_loader, val_loader, test_loader, num_classes = get_speechcommands_dataloaders(\n",
    "    config['data_folder'],\n",
    "    batch_size=config['batch_size']\n",
    ")\n",
    "\n",
    "# 4. Build model\n",
    "model = RadLIFNet(\n",
    "    input_size=40,  # log-Mel spectrogram features\n",
    "    hidden_sizes=config['layer_sizes'],\n",
    "    output_size=num_classes,\n",
    "    dropout=config['dropout'],\n",
    "    normalization=config['normalization'],\n",
    "    bidirectional=config['bidirectional']\n",
    ").to(config['device'])\n",
    "\n",
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# 6. Train\n",
    "train_loss, train_acc, val_loss, val_acc, spike_rates, total_spikes = trainer.train()\n",
    "\n",
    "# 7. Evaluate\n",
    "print(\"\\nTest Accuracy:\", trainer.evaluate())\n",
    "\n",
    "# 8. Plot results\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_loss, label='Train Loss')\n",
    "plt.plot(val_loss, label='Val Loss')\n",
    "plt.title(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_acc, label='Train Acc')\n",
    "plt.plot(val_acc, label='Val Acc')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(spike_rates, label='Spike Rate')\n",
    "plt.plot(total_spikes, label='Total Spikes')\n",
    "plt.title(\"Spike Activity\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Ã— git clone --filter=blob:none --quiet https://github.com/SelinaJiang/SpArch.git 'C:\\Users\\Aswin Kumar\\AppData\\Local\\Temp\\pip-req-build-f5ttamej' did not run successfully.\n",
      "  â”‚ exit code: 128\n",
      "  â•°â”€> [2 lines of output]\n",
      "      remote: Repository not found.\n",
      "      fatal: repository 'https://github.com/SelinaJiang/SpArch.git/' not found\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Ã— git clone --filter=blob:none --quiet https://github.com/SelinaJiang/SpArch.git 'C:\\Users\\Aswin Kumar\\AppData\\Local\\Temp\\pip-req-build-f5ttamej' did not run successfully.\n",
      "â”‚ exit code: 128\n",
      "â•°â”€> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sparch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msparch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_dataset\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msparch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_model\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msparch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sparch'"
     ]
    }
   ],
   "source": [
    "# Google Speech Commands - Baseline SNN using SpArch\n",
    "\n",
    "# Install and import required libraries\n",
    "!pip install torch torchaudio numpy matplotlib --quiet\n",
    "!pip install git+https://github.com/SelinaJiang/SpArch.git --quiet\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sparch.datasets import get_dataset\n",
    "from sparch.models import get_model\n",
    "from sparch.utils.train import train\n",
    "from sparch.utils.eval import evaluate\n",
    "from sparch.utils.misc import count_spikes\n",
    "\n",
    "# Dataset setup\n",
    "DATA_FOLDER = './speech_commands'\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.makedirs(DATA_FOLDER)\n",
    "\n",
    "# Download the dataset (optional, if you haven't already downloaded it manually)\n",
    "torchaudio.datasets.SPEECHCOMMANDS(root=DATA_FOLDER, download=True)\n",
    "\n",
    "# Hyperparameters\n",
    "params = {\n",
    "    'model_type': 'RadLIF',\n",
    "    'dataset_name': 'sc',\n",
    "    'data_folder': DATA_FOLDER,\n",
    "    'batch_size': 64,\n",
    "    'nb_epochs': 100,\n",
    "    'lr': 1e-3,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'weight_decay': 1e-4,\n",
    "    'dropout': 0.2,\n",
    "    'layer_sizes': [256, 256, 128],\n",
    "    'bidirectional': True,\n",
    "    'normalization': 'layernorm',\n",
    "    'optimizer': 'adamw',\n",
    "    'scheduler': 'cosine',\n",
    "    't_max': 100,\n",
    "    'exp_folder': './experiments/gsc_baseline',\n",
    "}\n",
    "\n",
    "# Load dataset\n",
    "data = get_dataset(\n",
    "    dataset_name=params['dataset_name'],\n",
    "    data_folder=params['data_folder'],\n",
    "    batch_size=params['batch_size']\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = get_model(\n",
    "    model_type=params['model_type'],\n",
    "    input_shape=data['input_shape'],\n",
    "    nb_classes=data['nb_classes'],\n",
    "    layer_sizes=params['layer_sizes'],\n",
    "    bidirectional=params['bidirectional'],\n",
    "    dropout=params['dropout'],\n",
    "    normalization=params['normalization']\n",
    ")\n",
    "\n",
    "# Training\n",
    "train(\n",
    "    model=model,\n",
    "    train_loader=data['train_loader'],\n",
    "    valid_loader=data['valid_loader'],\n",
    "    nb_epochs=params['nb_epochs'],\n",
    "    optimizer_name=params['optimizer'],\n",
    "    scheduler_name=params['scheduler'],\n",
    "    lr=params['lr'],\n",
    "    weight_decay=params['weight_decay'],\n",
    "    beta1=params['beta1'],\n",
    "    beta2=params['beta2'],\n",
    "    t_max=params['t_max'],\n",
    "    exp_folder=params['exp_folder']\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = evaluate(\n",
    "    model=model,\n",
    "    data_loader=data['test_loader'],\n",
    "    nb_classes=data['nb_classes'],\n",
    "    exp_folder=params['exp_folder']\n",
    ")\n",
    "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# Measure spike activity\n",
    "print(\"\\nMeasuring spike activity...\")\n",
    "model.eval()\n",
    "spike_info = count_spikes(\n",
    "    model=model,\n",
    "    data_loader=data['test_loader'],\n",
    "    nb_batches=10  # You can increase this for more accuracy\n",
    ")\n",
    "print(f\"Average Spike Rate per Neuron: {spike_info['spike_rate']:.4f}\")\n",
    "print(f\"Total Spikes: {spike_info['total_spikes']}\")\n",
    "\n",
    "# Plot training results\n",
    "log_path = os.path.join(params['exp_folder'], 'train_log.npy')\n",
    "if os.path.exists(log_path):\n",
    "    logs = np.load(log_path, allow_pickle=True).item()\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(logs['train_acc'], label='Train Acc')\n",
    "    plt.plot(logs['val_acc'], label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.title('Accuracy')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(logs['train_loss'], label='Train Loss')\n",
    "    plt.plot(logs['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss')\n",
    "    plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    \n",
    "    print(\"Training logs not found. Skipping plots.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
