{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from snntorch import surrogate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyperparameters (matched with DSC-SNN)\n",
    "batch_size = 64\n",
    "num_steps = 25\n",
    "num_inputs = 28 * 28\n",
    "num_hidden = 100\n",
    "num_outputs = 10\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Device handling\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Surrogate gradient\n",
    "spike_grad = surrogate.fast_sigmoid(slope=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # SNN with DSC\n",
    "class SNNWithDSC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "        # Enhanced threshold networks with conservative initialization\n",
    "        self.threshold_net1 = nn.Sequential(\n",
    "            nn.Linear(num_inputs, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, num_hidden),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.threshold_net2 = nn.Sequential(\n",
    "            nn.Linear(num_hidden, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, num_outputs),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.lif1 = snn.Leaky(beta=0.9, spike_grad=spike_grad)\n",
    "        self.lif2 = snn.Leaky(beta=0.9, spike_grad=spike_grad)\n",
    "        # Initialize with lower weights\n",
    "        self.threshold_net1.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        self.threshold_net2.apply(lambda m: m.reset_parameters() if hasattr(m, 'reset_parameters') else None)\n",
    "        for net in [self.threshold_net1, self.threshold_net2]:\n",
    "            for layer in net:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(layer.weight, gain=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(1)\n",
    "        mem1, mem2 = self.lif1.init_leaky(), self.lif2.init_leaky()\n",
    "        spk1_rec, spk2_rec = [], []\n",
    "        for step in range(num_steps):\n",
    "            scale1 = 1.0 - self.threshold_net1(x[step])\n",
    "            cur1 = self.fc1(x[step]) * scale1.clamp(min=0.1)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            scale2 = 1.0 - self.threshold_net2(spk1)\n",
    "            cur2 = self.fc2(spk1) * scale2.clamp(min=0.1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            spk1_rec.append(spk1)\n",
    "            spk2_rec.append(spk2)\n",
    "        return torch.stack(spk1_rec, dim=0), torch.stack(spk2_rec, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate network and optimizer\n",
    "net = BaselineSNN().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1191, Spike Rate: 0.1152, Accuracy: 0.9375, Validation Accuracy: 0.9469, Total Spikes: 3385166.0\n",
      "Epoch 2, Loss: 0.1177, Spike Rate: 0.1213, Accuracy: 0.9688, Validation Accuracy: 0.9623, Total Spikes: 3402440.0\n",
      "Epoch 3, Loss: 0.2110, Spike Rate: 0.1200, Accuracy: 0.9062, Validation Accuracy: 0.9639, Total Spikes: 3236194.0\n",
      "Epoch 4, Loss: 0.0126, Spike Rate: 0.1274, Accuracy: 1.0000, Validation Accuracy: 0.9700, Total Spikes: 3472396.0\n",
      "Epoch 5, Loss: 0.2552, Spike Rate: 0.1106, Accuracy: 0.9688, Validation Accuracy: 0.9709, Total Spikes: 3127967.0\n",
      "Epoch 6, Loss: 0.0045, Spike Rate: 0.1130, Accuracy: 1.0000, Validation Accuracy: 0.9762, Total Spikes: 3282026.0\n",
      "Epoch 7, Loss: 0.0036, Spike Rate: 0.1326, Accuracy: 1.0000, Validation Accuracy: 0.9735, Total Spikes: 3454707.0\n",
      "Epoch 8, Loss: 0.0033, Spike Rate: 0.1302, Accuracy: 1.0000, Validation Accuracy: 0.9743, Total Spikes: 3477529.0\n",
      "Epoch 9, Loss: 0.0120, Spike Rate: 0.1288, Accuracy: 1.0000, Validation Accuracy: 0.9737, Total Spikes: 3466902.0\n",
      "Epoch 10, Loss: 0.0032, Spike Rate: 0.1342, Accuracy: 1.0000, Validation Accuracy: 0.9767, Total Spikes: 3506348.0\n"
     ]
    }
   ],
   "source": [
    "# Training and evaluation\n",
    "accuracies, val_accuracies, spike_rates, losses = [], [], [], []\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    for data, targets in train_loader:\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        batch_size_actual = data.size(0)\n",
    "        data = data.view(batch_size_actual, -1)\n",
    "        if data.max() > 0:\n",
    "            data = data / data.max()\n",
    "        spike_data = (torch.rand(num_steps, batch_size_actual, num_inputs, device=device) < data * 0.3).float()\n",
    "        spk1_rec, outputs = net(spike_data)\n",
    "        spk_count = outputs.sum(dim=0)\n",
    "        loss = criterion(spk_count, targets)\n",
    "        spike_rate = (spk1_rec.mean() + outputs.mean()) / 2\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        correct = (spk_count.argmax(dim=1) == targets).sum().item()\n",
    "        accuracy = correct / batch_size_actual\n",
    "    scheduler.step()\n",
    "    losses.append(loss.item())\n",
    "    spike_rates.append(spike_rate.item())\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    # Validation\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    total_spikes = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            batch_size_actual = data.size(0)\n",
    "            data = data.view(batch_size_actual, -1)\n",
    "            if data.max() > 0:\n",
    "                data = data / data.max()\n",
    "            spike_data = (torch.rand(num_steps, batch_size_actual, num_inputs, device=device) < data * 0.3).float()\n",
    "            spk1_rec, outputs = net(spike_data)\n",
    "            spk_count = outputs.sum(dim=0)\n",
    "            total_correct += (spk_count.argmax(dim=1) == targets).sum().item()\n",
    "            total_samples += batch_size_actual\n",
    "            total_spikes += (spk1_rec.sum() + outputs.sum()).item()\n",
    "        val_accuracy = total_correct / total_samples\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}, Spike Rate: {spike_rate.item():.4f}, Accuracy: {accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}, Total Spikes: {total_spikes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
